{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import cv2\n",
    "import  torch\n",
    "import  numpy as np\n",
    "import torch.utils.data as data\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "import pandas as pd\n",
    "import random\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Imgdataset(data.Dataset):\n",
    "    def __init__(self, img_folder, ann_data, img_size=64, transform=[]):\n",
    "        self.img_folder = img_folder\n",
    "        self.ann_data = ann_data\n",
    "        self.transform = transform\n",
    "\n",
    "        self.all_ann_smiling = {}\n",
    "        with open(ann_data, 'r') as f:\n",
    "            rows = csv.DictReader(f)\n",
    "            for row in rows:\n",
    "                self.all_ann_smiling[row['image_name']] = int(float(row['Smiling']))\n",
    "        \n",
    "        self.all_img_path = []\n",
    "        all_img_file = os.listdir(img_folder)\n",
    "        for file_name in all_img_file:\n",
    "            self.all_img_path.append(file_name)\n",
    "\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.all_img_path)   \n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        img_file_name = self.all_img_path[index]\n",
    "        label = self.all_ann_smiling[img_file_name]\n",
    "        \n",
    "        img_path = os.path.join(self.img_folder, img_file_name)\n",
    "        img = cv2.imread(img_path)\n",
    "        img = self.BGR2RGB(img)\n",
    "        #img = self.random_flip(img)\n",
    "        for t in self.transform:\n",
    "            img = t(img)\n",
    "\n",
    "        return img, label\n",
    "\n",
    "\n",
    "    def BGR2RGB(self,img):\n",
    "        return cv2.cvtColor(img,cv2.COLOR_BGR2RGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, figsize=64):\n",
    "        super(Generator, self).__init__()\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d( 100, figsize * 8, 4, 1, 0, bias=False),\n",
    "            nn.BatchNorm2d(figsize * 8),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.ConvTranspose2d(figsize * 8, figsize * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(figsize * 4),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.ConvTranspose2d(figsize * 4, figsize * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(figsize * 2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.ConvTranspose2d(figsize * 2, figsize, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(figsize),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.ConvTranspose2d(figsize, 3, 4, 2, 1, bias=False),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, X):\n",
    "        output = self.decoder(X)/2.0+0.5\n",
    "        return output\n",
    "    \n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, figsize=64):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Conv2d(3, figsize, 4, 2, 1, bias=False),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(figsize, figsize * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(figsize * 2),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(figsize * 2, figsize * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(figsize * 4),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(figsize * 4, figsize * 8, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(figsize * 8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(figsize * 8, 1, 4, 1, 0, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, X):\n",
    "        output = self.decoder(X)\n",
    "        #print(output.size())\n",
    "        return output.squeeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed((38))\n",
    "torch.manual_seed(38)\n",
    "fixed_noise = torch.randn(32, 100, 1, 1)\n",
    "fixed_noise = Variable(fixed_noise).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file_img = \"../../../hw3_data/face/train\"\n",
    "csv_data_path = \"../../../hw3_data/face/train.csv\"\n",
    "img_size = 64\n",
    "train_data = Imgdataset(train_file_img, csv_data_path, img_size, transform=[transforms.ToTensor()])\n",
    "\n",
    "dataloader = torch.utils.data.DataLoader(\n",
    "                                dataset=train_data,\n",
    "                                batch_size=64,\n",
    "                                shuffle=True,\n",
    "                                num_workers=8\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "training D Loss: 0.8201619282126427\n",
      "training G Loss: 3.1805259925842284\n",
      "D_real_acc: 51.752\n",
      "D_fake_acc: 53.6416\n",
      "Epoch: 2\n",
      "training D Loss: 0.7773810708999633\n",
      "training G Loss: 3.1401797554016113\n",
      "D_real_acc: 52.408\n",
      "D_fake_acc: 54.0208\n",
      "Epoch: 3\n",
      "training D Loss: 0.7060195737838745\n",
      "training G Loss: 3.0358400404930115\n",
      "D_real_acc: 54.216\n",
      "D_fake_acc: 55.1072\n",
      "Epoch: 4\n",
      "training D Loss: 0.759030563545227\n",
      "training G Loss: 2.760032010555267\n",
      "D_real_acc: 53.08\n",
      "D_fake_acc: 53.688\n",
      "Epoch: 5\n",
      "training D Loss: 0.7813392647743225\n",
      "training G Loss: 2.580987536239624\n",
      "D_real_acc: 52.8352\n",
      "D_fake_acc: 53.7088\n",
      "Epoch: 6\n",
      "training D Loss: 0.7429836676597595\n",
      "training G Loss: 2.5491879749298096\n",
      "D_real_acc: 53.6224\n",
      "D_fake_acc: 54.0208\n",
      "Epoch: 7\n",
      "training D Loss: 0.7133242322444916\n",
      "training G Loss: 2.494649712228775\n",
      "D_real_acc: 54.1248\n",
      "D_fake_acc: 54.6544\n",
      "Epoch: 8\n",
      "training D Loss: 0.6856780827045441\n",
      "training G Loss: 2.550289152240753\n",
      "D_real_acc: 54.7696\n",
      "D_fake_acc: 55.2592\n",
      "Epoch: 9\n",
      "training D Loss: 0.6540602670669555\n",
      "training G Loss: 2.5720846733093263\n",
      "D_real_acc: 55.4096\n",
      "D_fake_acc: 55.8208\n",
      "Epoch: 10\n",
      "training D Loss: 0.6241034541130066\n",
      "training G Loss: 2.6798250378131865\n",
      "D_real_acc: 55.9888\n",
      "D_fake_acc: 56.2992\n",
      "Epoch: 11\n",
      "learning rate change!\n",
      "training D Loss: 0.3791054280519485\n",
      "training G Loss: 2.428287067127228\n",
      "D_real_acc: 61.64\n",
      "D_fake_acc: 62.1408\n",
      "Epoch: 12\n",
      "training D Loss: 0.3885620098114014\n",
      "training G Loss: 2.596766681480408\n",
      "D_real_acc: 60.7536\n",
      "D_fake_acc: 61.0496\n",
      "Epoch: 13\n",
      "training D Loss: 0.4018689594268799\n",
      "training G Loss: 2.7530755479335784\n",
      "D_real_acc: 60.3776\n",
      "D_fake_acc: 60.6176\n",
      "Epoch: 14\n",
      "training D Loss: 0.3397895138978958\n",
      "training G Loss: 2.8162140395164488\n",
      "D_real_acc: 61.3664\n",
      "D_fake_acc: 61.3392\n",
      "Epoch: 15\n",
      "training D Loss: 0.3385074956178665\n",
      "training G Loss: 2.9469771111011505\n",
      "D_real_acc: 61.1984\n",
      "D_fake_acc: 61.2032\n",
      "Epoch: 16\n",
      "training D Loss: 0.31396208276748655\n",
      "training G Loss: 3.083029835510254\n",
      "D_real_acc: 61.536\n",
      "D_fake_acc: 61.5648\n",
      "Epoch: 17\n",
      "training D Loss: 0.30285152155160905\n",
      "training G Loss: 3.140218178319931\n",
      "D_real_acc: 61.6736\n",
      "D_fake_acc: 61.5264\n",
      "Epoch: 18\n",
      "training D Loss: 0.3238676782131195\n",
      "training G Loss: 3.229959437692165\n",
      "D_real_acc: 61.2144\n",
      "D_fake_acc: 61.2832\n",
      "Epoch: 19\n",
      "training D Loss: 0.2505213113546371\n",
      "training G Loss: 3.3318114108085632\n",
      "D_real_acc: 62.1488\n",
      "D_fake_acc: 62.2368\n",
      "Epoch: 20\n",
      "learning rate change!\n",
      "training D Loss: 0.15634996015429498\n",
      "training G Loss: 3.3534589710235596\n",
      "D_real_acc: 63.7136\n",
      "D_fake_acc: 63.6896\n",
      "Epoch: 21\n",
      "training D Loss: 0.16383748193979264\n",
      "training G Loss: 3.4441543821334837\n",
      "D_real_acc: 63.4752\n",
      "D_fake_acc: 63.3648\n",
      "Epoch: 22\n",
      "training D Loss: 0.1490102904379368\n",
      "training G Loss: 3.5666560827255247\n",
      "D_real_acc: 63.5936\n",
      "D_fake_acc: 63.4912\n",
      "Epoch: 23\n",
      "training D Loss: 0.14713027234077453\n",
      "training G Loss: 3.6977018249511717\n",
      "D_real_acc: 63.5984\n",
      "D_fake_acc: 63.3616\n",
      "Epoch: 24\n",
      "training D Loss: 0.16083265169858932\n",
      "training G Loss: 3.7558328259944918\n",
      "D_real_acc: 63.2816\n",
      "D_fake_acc: 63.1456\n",
      "Epoch: 25\n",
      "training D Loss: 0.13971428485810758\n",
      "training G Loss: 3.814996150135994\n",
      "D_real_acc: 63.4928\n",
      "D_fake_acc: 63.2896\n",
      "Epoch: 26\n",
      "training D Loss: 0.17355265562534333\n",
      "training G Loss: 3.8646919171899556\n",
      "D_real_acc: 63.0608\n",
      "D_fake_acc: 62.8656\n",
      "Epoch: 27\n",
      "training D Loss: 0.11271590062379837\n",
      "training G Loss: 3.9445663262367248\n",
      "D_real_acc: 63.76\n",
      "D_fake_acc: 63.6656\n",
      "Epoch: 28\n",
      "training D Loss: 0.126402839076519\n",
      "training G Loss: 4.035244692802429\n",
      "D_real_acc: 63.5856\n",
      "D_fake_acc: 63.3984\n",
      "Epoch: 29\n",
      "training D Loss: 0.154507069632411\n",
      "training G Loss: 4.078468266391754\n",
      "D_real_acc: 63.2672\n",
      "D_fake_acc: 62.9904\n",
      "Epoch: 30\n",
      "training D Loss: 0.10160876069068908\n",
      "training G Loss: 4.112809097671509\n",
      "D_real_acc: 63.8464\n",
      "D_fake_acc: 63.7264\n",
      "Epoch: 31\n",
      "training D Loss: 0.15394588719904423\n",
      "training G Loss: 4.089782279253006\n",
      "D_real_acc: 63.1248\n",
      "D_fake_acc: 62.9664\n",
      "Epoch: 32\n",
      "training D Loss: 0.09877007909119129\n",
      "training G Loss: 4.210751851081848\n",
      "D_real_acc: 63.8384\n",
      "D_fake_acc: 63.6768\n",
      "Epoch: 33\n",
      "training D Loss: 0.17890386522114277\n",
      "training G Loss: 4.141030330324173\n",
      "D_real_acc: 62.7968\n",
      "D_fake_acc: 62.5808\n",
      "Epoch: 34\n",
      "training D Loss: 0.13877742337584495\n",
      "training G Loss: 4.115578683042526\n",
      "D_real_acc: 63.2304\n",
      "D_fake_acc: 63.1808\n",
      "Epoch: 35\n",
      "training D Loss: 0.0913555340975523\n",
      "training G Loss: 4.274307293510437\n",
      "D_real_acc: 63.872\n",
      "D_fake_acc: 63.7552\n",
      "Epoch: 36\n",
      "training D Loss: 0.16010910471379758\n",
      "training G Loss: 4.137910405641795\n",
      "D_real_acc: 63.12\n",
      "D_fake_acc: 62.9456\n",
      "Epoch: 37\n",
      "training D Loss: 0.11656633086204529\n",
      "training G Loss: 4.278940116214752\n",
      "D_real_acc: 63.5536\n",
      "D_fake_acc: 63.3792\n",
      "Epoch: 38\n",
      "training D Loss: 0.16672413652837276\n",
      "training G Loss: 4.225315587615967\n",
      "D_real_acc: 62.9024\n",
      "D_fake_acc: 62.7056\n",
      "Epoch: 39\n",
      "training D Loss: 0.1223996396869421\n",
      "training G Loss: 4.2469727373123165\n",
      "D_real_acc: 63.4064\n",
      "D_fake_acc: 63.2736\n",
      "Epoch: 40\n",
      "training D Loss: 0.09203163847923279\n",
      "training G Loss: 4.331512017059326\n",
      "D_real_acc: 63.8688\n",
      "D_fake_acc: 63.7664\n",
      "Epoch: 41\n",
      "training D Loss: 0.19275837895274162\n",
      "training G Loss: 4.154517020711303\n",
      "D_real_acc: 62.76\n",
      "D_fake_acc: 62.5744\n",
      "Epoch: 42\n",
      "training D Loss: 0.09052203173339367\n",
      "training G Loss: 4.396385831832886\n",
      "D_real_acc: 63.8816\n",
      "D_fake_acc: 63.6672\n",
      "Epoch: 43\n",
      "training D Loss: 0.14928062040507795\n",
      "training G Loss: 4.29109326159954\n",
      "D_real_acc: 63.0896\n",
      "D_fake_acc: 62.9904\n",
      "Epoch: 44\n",
      "training D Loss: 0.12576550098061562\n",
      "training G Loss: 4.350646141338348\n",
      "D_real_acc: 63.3936\n",
      "D_fake_acc: 63.216\n",
      "Epoch: 45\n",
      "training D Loss: 0.10069512805640697\n",
      "training G Loss: 4.450841065979004\n",
      "D_real_acc: 63.6272\n",
      "D_fake_acc: 63.5024\n",
      "Epoch: 46\n",
      "training D Loss: 0.14971479573845864\n",
      "training G Loss: 4.36084657535553\n",
      "D_real_acc: 63.0592\n",
      "D_fake_acc: 62.9232\n",
      "Epoch: 47\n",
      "training D Loss: 0.08646326985508203\n",
      "training G Loss: 4.505166715621948\n",
      "D_real_acc: 63.8464\n",
      "D_fake_acc: 63.6992\n",
      "Epoch: 48\n",
      "training D Loss: 0.15208436277210713\n",
      "training G Loss: 4.338519136714935\n",
      "D_real_acc: 63.1232\n",
      "D_fake_acc: 62.9296\n",
      "Epoch: 49\n",
      "training D Loss: 0.11215147993862629\n",
      "training G Loss: 4.506279532051086\n",
      "D_real_acc: 63.4912\n",
      "D_fake_acc: 63.32\n",
      "Epoch: 50\n",
      "training D Loss: 0.153271538823843\n",
      "training G Loss: 4.416574397295713\n",
      "D_real_acc: 63.0896\n",
      "D_fake_acc: 62.9248\n",
      "Epoch: 51\n",
      "training D Loss: 0.14139573662281035\n",
      "training G Loss: 4.425181707596779\n",
      "D_real_acc: 63.0992\n",
      "D_fake_acc: 62.904\n",
      "Epoch: 52\n",
      "training D Loss: 0.08144616759866476\n",
      "training G Loss: 4.479546161842346\n",
      "D_real_acc: 63.8944\n",
      "D_fake_acc: 63.7808\n",
      "Epoch: 53\n",
      "training D Loss: 0.12346962120383978\n",
      "training G Loss: 4.588406427907944\n",
      "D_real_acc: 63.4048\n",
      "D_fake_acc: 63.2112\n",
      "Epoch: 54\n",
      "training D Loss: 0.13053810647428035\n",
      "training G Loss: 4.509407236862183\n",
      "D_real_acc: 63.264\n",
      "D_fake_acc: 63.0528\n",
      "Epoch: 55\n",
      "training D Loss: 0.08778811509907246\n",
      "training G Loss: 4.646753565144539\n",
      "D_real_acc: 63.6992\n",
      "D_fake_acc: 63.5984\n",
      "Epoch: 56\n",
      "training D Loss: 0.13447843416780234\n",
      "training G Loss: 4.443452391242981\n",
      "D_real_acc: 63.2064\n",
      "D_fake_acc: 63.0144\n",
      "Epoch: 57\n",
      "training D Loss: 0.11232863847017288\n",
      "training G Loss: 4.612594908332825\n",
      "D_real_acc: 63.3856\n",
      "D_fake_acc: 63.2128\n",
      "Epoch: 58\n",
      "training D Loss: 0.10755732645392418\n",
      "training G Loss: 4.620983581924438\n",
      "D_real_acc: 63.4304\n",
      "D_fake_acc: 63.2848\n",
      "Epoch: 59\n",
      "training D Loss: 0.13652883930057289\n",
      "training G Loss: 4.591121140623093\n",
      "D_real_acc: 63.2192\n",
      "D_fake_acc: 63.0736\n",
      "Epoch: 60\n",
      "training D Loss: 0.07904911386370658\n",
      "training G Loss: 4.711262213420868\n",
      "D_real_acc: 63.768\n",
      "D_fake_acc: 63.664\n",
      "Epoch: 61\n",
      "training D Loss: 0.09533549335300923\n",
      "training G Loss: 4.71646549949646\n",
      "D_real_acc: 63.608\n",
      "D_fake_acc: 63.4624\n",
      "Epoch: 62\n",
      "training D Loss: 0.12123657995462418\n",
      "training G Loss: 4.644762402629852\n",
      "D_real_acc: 63.232\n",
      "D_fake_acc: 63.0576\n",
      "Epoch: 63\n",
      "training D Loss: 0.13347528220415114\n",
      "training G Loss: 4.6692362057328225\n",
      "D_real_acc: 63.2512\n",
      "D_fake_acc: 63.12\n",
      "Epoch: 64\n",
      "training D Loss: 0.14816709328442812\n",
      "training G Loss: 4.714755044847727\n",
      "D_real_acc: 63.1168\n",
      "D_fake_acc: 62.9632\n",
      "Epoch: 65\n",
      "training D Loss: 0.07582005086541176\n",
      "training G Loss: 4.698673241806031\n",
      "D_real_acc: 63.8624\n",
      "D_fake_acc: 63.7456\n",
      "Epoch: 66\n",
      "training D Loss: 0.13485232860744\n",
      "training G Loss: 4.673973990869522\n",
      "D_real_acc: 63.1792\n",
      "D_fake_acc: 63.0384\n",
      "Epoch: 67\n",
      "training D Loss: 0.11791745449006558\n",
      "training G Loss: 4.718726209926605\n",
      "D_real_acc: 63.3712\n",
      "D_fake_acc: 63.2464\n",
      "Epoch: 68\n",
      "training D Loss: 0.1202118936508894\n",
      "training G Loss: 4.745167853021622\n",
      "D_real_acc: 63.3104\n",
      "D_fake_acc: 63.1632\n",
      "Epoch: 69\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training D Loss: 0.07445280629992485\n",
      "training G Loss: 4.814301338958741\n",
      "D_real_acc: 63.8816\n",
      "D_fake_acc: 63.7296\n",
      "Epoch: 70\n",
      "training D Loss: 0.1405616348683834\n",
      "training G Loss: 4.758867382198572\n",
      "D_real_acc: 63.1744\n",
      "D_fake_acc: 63.0288\n",
      "Epoch: 71\n",
      "training D Loss: 0.13508083447664976\n",
      "training G Loss: 4.7202636922836305\n",
      "D_real_acc: 63.2432\n",
      "D_fake_acc: 63.0416\n",
      "Epoch: 72\n",
      "training D Loss: 0.07246173892766238\n",
      "training G Loss: 4.875321855926513\n",
      "D_real_acc: 63.8208\n",
      "D_fake_acc: 63.7056\n",
      "Epoch: 73\n",
      "training D Loss: 0.15760874087512494\n",
      "training G Loss: 4.665402967396378\n",
      "D_real_acc: 62.9856\n",
      "D_fake_acc: 62.8336\n",
      "Epoch: 74\n",
      "training D Loss: 0.12501255050748586\n",
      "training G Loss: 4.841278453826904\n",
      "D_real_acc: 63.136\n",
      "D_fake_acc: 63.0448\n",
      "Epoch: 75\n",
      "training D Loss: 0.09864634716808796\n",
      "training G Loss: 4.651648036336899\n",
      "D_real_acc: 63.4944\n",
      "D_fake_acc: 63.4208\n",
      "Epoch: 76\n",
      "training D Loss: 0.089897746104002\n",
      "training G Loss: 4.896641481590271\n",
      "D_real_acc: 63.6128\n",
      "D_fake_acc: 63.4192\n",
      "Epoch: 77\n",
      "training D Loss: 0.12838010156452656\n",
      "training G Loss: 4.843380849885941\n",
      "D_real_acc: 63.1712\n",
      "D_fake_acc: 62.9056\n",
      "Epoch: 78\n",
      "training D Loss: 0.10815517362654209\n",
      "training G Loss: 4.826292799186707\n",
      "D_real_acc: 63.5184\n",
      "D_fake_acc: 63.3888\n",
      "Epoch: 79\n",
      "training D Loss: 0.12637073289901019\n",
      "training G Loss: 4.795397810872644\n",
      "D_real_acc: 63.32\n",
      "D_fake_acc: 63.2288\n",
      "Epoch: 80\n",
      "training D Loss: 0.09193353305533529\n",
      "training G Loss: 4.89010190911293\n",
      "D_real_acc: 63.5408\n",
      "D_fake_acc: 63.4128\n",
      "Epoch: 81\n",
      "training D Loss: 0.0672082689806819\n",
      "training G Loss: 5.0394805837631225\n",
      "D_real_acc: 63.8768\n",
      "D_fake_acc: 63.7392\n",
      "Epoch: 82\n",
      "training D Loss: 0.13444370645731687\n",
      "training G Loss: 4.8400674109220505\n",
      "D_real_acc: 63.1824\n",
      "D_fake_acc: 63.072\n",
      "Epoch: 83\n",
      "training D Loss: 0.11707752888649701\n",
      "training G Loss: 4.864699748134613\n",
      "D_real_acc: 63.3296\n",
      "D_fake_acc: 63.2368\n",
      "Epoch: 84\n",
      "training D Loss: 0.06352222636193038\n",
      "training G Loss: 5.05916515007019\n",
      "D_real_acc: 63.9232\n",
      "D_fake_acc: 63.7456\n",
      "Epoch: 85\n",
      "training D Loss: 0.16515551133453846\n",
      "training G Loss: 4.786216780316829\n",
      "D_real_acc: 62.7248\n",
      "D_fake_acc: 62.608\n",
      "Epoch: 86\n",
      "training D Loss: 0.08823069803863764\n",
      "training G Loss: 4.977728491258621\n",
      "D_real_acc: 63.5824\n",
      "D_fake_acc: 63.464\n",
      "Epoch: 87\n",
      "training D Loss: 0.07652274755239487\n",
      "training G Loss: 4.942712852478027\n",
      "D_real_acc: 63.7664\n",
      "D_fake_acc: 63.6304\n",
      "Epoch: 88\n",
      "training D Loss: 0.1257878567710519\n",
      "training G Loss: 4.935034429198503\n",
      "D_real_acc: 63.1664\n",
      "D_fake_acc: 63.1008\n",
      "Epoch: 89\n",
      "training D Loss: 0.11183321079611779\n",
      "training G Loss: 4.908288330173493\n",
      "D_real_acc: 63.4016\n",
      "D_fake_acc: 63.2224\n",
      "Epoch: 90\n",
      "training D Loss: 0.05725344493538141\n",
      "training G Loss: 5.144117623710632\n",
      "D_real_acc: 63.9344\n",
      "D_fake_acc: 63.824\n",
      "Epoch: 91\n",
      "training D Loss: 0.11689321659505367\n",
      "training G Loss: 4.9748401741981505\n",
      "D_real_acc: 63.3616\n",
      "D_fake_acc: 63.1984\n",
      "Epoch: 92\n",
      "training D Loss: 0.13824706634357573\n",
      "training G Loss: 4.890179113036394\n",
      "D_real_acc: 63.1344\n",
      "D_fake_acc: 62.976\n",
      "Epoch: 93\n",
      "training D Loss: 0.09793118500560521\n",
      "training G Loss: 4.9503896493434905\n",
      "D_real_acc: 63.4896\n",
      "D_fake_acc: 63.4112\n",
      "Epoch: 94\n",
      "training D Loss: 0.11766920936405659\n",
      "training G Loss: 5.088673918235302\n",
      "D_real_acc: 63.2208\n",
      "D_fake_acc: 63.0816\n",
      "Epoch: 95\n",
      "training D Loss: 0.06881458094641567\n",
      "training G Loss: 5.038379566192627\n",
      "D_real_acc: 63.8688\n",
      "D_fake_acc: 63.6656\n",
      "Epoch: 96\n",
      "training D Loss: 0.09005561594218016\n",
      "training G Loss: 5.065003795528412\n",
      "D_real_acc: 63.4672\n",
      "D_fake_acc: 63.3328\n",
      "Epoch: 97\n",
      "training D Loss: 0.13102530859261752\n",
      "training G Loss: 5.022564605000615\n",
      "D_real_acc: 63.1664\n",
      "D_fake_acc: 63.0528\n",
      "Epoch: 98\n",
      "training D Loss: 0.0840223623752594\n",
      "training G Loss: 5.09208631067276\n",
      "D_real_acc: 63.5808\n",
      "D_fake_acc: 63.4288\n",
      "Epoch: 99\n",
      "training D Loss: 0.07519557532817125\n",
      "training G Loss: 5.179602286529541\n",
      "D_real_acc: 63.696\n",
      "D_fake_acc: 63.536\n",
      "Epoch: 100\n",
      "training D Loss: 0.09978117593377829\n",
      "training G Loss: 5.035033395123482\n",
      "D_real_acc: 63.4384\n",
      "D_fake_acc: 63.2816\n",
      "Epoch: 101\n",
      "training D Loss: 0.051006422534585\n",
      "training G Loss: 5.3196266128540035\n",
      "D_real_acc: 63.9536\n",
      "D_fake_acc: 63.8432\n",
      "Epoch: 102\n",
      "training D Loss: 0.13450484806895255\n",
      "training G Loss: 5.058720414039493\n",
      "D_real_acc: 63.2192\n",
      "D_fake_acc: 63.0176\n",
      "Epoch: 103\n",
      "training D Loss: 0.10694912204220891\n",
      "training G Loss: 5.17320956081152\n",
      "D_real_acc: 63.4064\n",
      "D_fake_acc: 63.1824\n",
      "Epoch: 104\n",
      "training D Loss: 0.11308014152646065\n",
      "training G Loss: 5.058521712827683\n",
      "D_real_acc: 63.2512\n",
      "D_fake_acc: 63.0912\n",
      "Epoch: 105\n",
      "training D Loss: 0.09942085009515285\n",
      "training G Loss: 5.081219366884231\n",
      "D_real_acc: 63.4336\n",
      "D_fake_acc: 63.3184\n"
     ]
    }
   ],
   "source": [
    "# training\n",
    "latent_size = 100\n",
    "\n",
    "G = Generator().cuda()\n",
    "D = Discriminator().cuda()\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "# setup optimizer\n",
    "optimizerG = optim.Adam(G.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "optimizerD = optim.Adam(D.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "D_loss_list = []\n",
    "G_loss_list = []\n",
    "D_fake_acc_list = []\n",
    "D_real_acc_list = []\n",
    "\n",
    "for epoch in range(105):\n",
    "    print(\"Epoch:\", epoch+1)\n",
    "    epoch_D_loss = 0.0\n",
    "    epoch_G_loss = 0.0\n",
    "    D_fake_acc = 0.0\n",
    "    D_real_acc = 0.0\n",
    "    \n",
    "    total_length = len(dataloader)\n",
    "    if (epoch+1) == 11:\n",
    "        optimizerG.param_groups[0]['lr'] /= 2\n",
    "        optimizerD.param_groups[0]['lr'] /= 2\n",
    "        print(\"learning rate change!\")\n",
    "\n",
    "    if (epoch+1) == 20:\n",
    "        optimizerG.param_groups[0]['lr'] /= 2\n",
    "        optimizerD.param_groups[0]['lr'] /= 2\n",
    "        print(\"learning rate change!\")\n",
    "        \n",
    "    for i, batch in enumerate(dataloader):\n",
    "        if i > total_length:\n",
    "            break\n",
    "            \n",
    "        D.zero_grad()\n",
    "        input_X = batch[0]\n",
    "        \n",
    "        real_image = Variable(input_X.cuda()) # use GPU \n",
    "        real_label = Variable(torch.ones((BATCH_SIZE))).cuda()\n",
    "        output = D(real_image)\n",
    "        #print(real_label.size())\n",
    "        #print(output)\n",
    "        \n",
    "        D_real_loss = criterion(output, real_label)\n",
    "        D_real_acc += np.mean(((output > 0.5).cpu().data.numpy() == real_label.cpu().data.numpy()))\n",
    "        \n",
    "        #### train with fake image -> ground truth = fake label\n",
    "        noise = Variable(torch.randn(BATCH_SIZE, 100, 1, 1)).cuda()\n",
    "        fake_image = G(noise)\n",
    "        fake_label = Variable(torch.zeros((BATCH_SIZE))).cuda()\n",
    "        output = D(fake_image.detach())\n",
    "        D_fake_loss = criterion(output, fake_label)\n",
    "        D_fake_acc += np.mean(((output > 0.5).cpu().data.numpy() == fake_label.cpu().data.numpy()))\n",
    "        \n",
    "        # update D\n",
    "        D_train_loss = D_real_loss + D_fake_loss\n",
    "        epoch_D_loss+=(D_train_loss.item())\n",
    "        D_train_loss.backward()\n",
    "        optimizerD.step()\n",
    "        \n",
    "        #### train Generator\n",
    "        for _ in range(1):\n",
    "            G.zero_grad()\n",
    "            # generate fake image\n",
    "            noise = Variable(torch.randn(BATCH_SIZE, 100, 1, 1)).cuda()\n",
    "            fake_image = G(noise)\n",
    "            fake_label_for_G = Variable(torch.ones((BATCH_SIZE))).cuda()\n",
    "            output = D(fake_image)\n",
    "            G_loss = criterion(output, fake_label_for_G)\n",
    "            epoch_G_loss += (G_loss.item())\n",
    "            G_loss.backward()\n",
    "            optimizerG.step()\n",
    "        \n",
    "    print(\"training D Loss:\",epoch_D_loss/(total_length))\n",
    "    print(\"training G Loss:\", epoch_G_loss/(total_length))\n",
    "    D_loss_list.append(epoch_D_loss/(total_length))\n",
    "    G_loss_list.append(epoch_G_loss/(total_length))\n",
    "    \n",
    "    print(\"D_real_acc:\", D_real_acc/(total_length/BATCH_SIZE))\n",
    "    print(\"D_fake_acc:\", D_fake_acc/(total_length/BATCH_SIZE))\n",
    "    \n",
    "    D_real_acc_list.append(D_real_acc/(total_length/BATCH_SIZE))\n",
    "    D_fake_acc_list.append(D_fake_acc/(total_length/BATCH_SIZE))\n",
    "    # evaluation\n",
    "    G.eval()\n",
    "    fixed_img_output = G(fixed_noise)\n",
    "    G.train()\n",
    "    torchvision.utils.save_image(fixed_img_output.cpu().data, './output_GAN/fig2_3_'+str(epoch+1)+'.jpg',nrow=8)\n",
    "    torch.save(G.state_dict(), \"./output_GAN/G_model.pkt.\"+str(epoch+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Load Model ###\n",
    "G = Generator()\n",
    "G.cuda()\n",
    "pretrained_acgan = \"./output_GAN/G_model.pkt.105\"\n",
    "G.load_state_dict(torch.load(pretrained_acgan))\n",
    "G.eval()\n",
    "\n",
    "random_seed = [72, 76, 88, 110, 159, 191, 198, 213,\n",
    "               229, 249, 303, 316, 338, 339, 348, 355, \n",
    "               363, 393, 414, 427, 430, 441, 447, 448, \n",
    "               547, 553, 555, 583, 632, 633, 673, 702]\n",
    "output = []\n",
    "for seed in random_seed:\n",
    "    \n",
    "    random.seed((seed))\n",
    "    torch.manual_seed(seed)\n",
    "    fixed_noise = torch.randn(1, 100, 1, 1)\n",
    "    fixed_noise = Variable(fixed_noise).cuda()\n",
    "    \n",
    "    fixed_img_output = G(fixed_noise)\n",
    "    output.append(fixed_img_output.cpu().data)\n",
    "output = torch.cat(output, 0)\n",
    "torchvision.utils.save_image(output, './GAN_seed/seed_'+\"ALL\"+'.jpg',nrow=8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
